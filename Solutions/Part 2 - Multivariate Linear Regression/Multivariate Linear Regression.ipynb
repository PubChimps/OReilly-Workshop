{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Machine Learning Tutorials with Watson Machine Learning\n",
    "### Part 2 - Multivariate Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for use in IBM Data Science Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from repository.mlrepositoryclient import MLRepositoryClient\n",
    "from repository.mlrepositoryartifact import MLRepositoryArtifact\n",
    "\n",
    "import urllib3, requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [num / 1.0 for num in range(100)]\n",
    "x2 = [num / 1.0 for num in range(100)]\n",
    "x3 = [num / 1.0 for num in range(100)]\n",
    "y = range(100)\n",
    "\n",
    "#creating a noisy function to y = x1 + 2x2 + 3x3\n",
    "for i in range(0,100):\n",
    "    y[i] = x1[i] + random.random() * random.uniform(-1.5,1.5) + \\\n",
    "           2 * x2[i] + random.random() * random.uniform(-1.5,1.5) + \\\n",
    "           3 * x3[i] + random.random() * random.uniform(-1.5,1.5)\n",
    "\n",
    "xytuple = zip(y,x1,x2,x3)\n",
    "\n",
    "#defining schema for spark dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"x1\", DoubleType(), True),\n",
    "    StructField(\"x2\", DoubleType(), True),\n",
    "    StructField(\"x3\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#creating spark dataframe, splitting to three groups\n",
    "df = sqlContext.createDataFrame(xytuple, schema)\n",
    "trainingdf, test1df, test2df = df.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainingdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Spark ML Pipeline\n",
    "group x values into single feature vector this will be beneficial in the next exercise, involving multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresvector = VectorAssembler(inputCols=[\"x1\", \"x2\", \"x3\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create linear regression instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple ML steps can be executed at once by utilizing a pipeline, here we build out a pipeline that combines features in a single vector, then executes a linear regression model on the resulting (label, feature). We then run this pipeline using our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages =[featuresvector, lr])\n",
    "#pipelinemodel = pipeline.fit(trainingdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use model on test1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelinemodel.transform(test1df)\n",
    "predictions.select('label','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Exercise\n",
    "#### The code above perform a linear regression training on data of the order y=x1 + 2x2 + 3x3 , plus some noise. Create data frames and deploy a model on the function y = -x1 - 5x2 - 10x3\n",
    "\n",
    "#### From the above code, what needs to be changed, what can stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create data the looks like y = -x1 - 5x2 - 10x3, plus some noise\n",
    "x1 = [num / 1.0 for num in range(100)]\n",
    "x2 = [num / 1.0 for num in range(100)]\n",
    "x3 = [num / 1.0 for num in range(100)]\n",
    "y = range(100)\n",
    "\n",
    "#creating a noisy function to y = -x1 - 5x2 - 10x3\n",
    "for i in range(0,100):\n",
    "    y[i] = x1[i] + random.random() * random.uniform(-1.5,1.5) - \\\n",
    "           5 * x2[i] + random.random() * random.uniform(-1.5,1.5) - \\\n",
    "           10 * x3[i] + random.random() * random.uniform(-1.5,1.5)\n",
    "            \n",
    "xytuple = zip(y,x1,x2,x3)\n",
    "\n",
    "### convert to three pyspark dataframes: newtrainingdf, newtest1df, & newtest2df\n",
    "#defining schema for spark dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"x1\", DoubleType(), True),\n",
    "    StructField(\"x2\", DoubleType(), True),\n",
    "    StructField(\"x3\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#creating spark dataframe, splitting to three groups\n",
    "df = sqlContext.createDataFrame(xytuple, schema)\n",
    "newtrainingdf, newtest1df, newtest2df = df.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "newtrainingdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model pipeline, pipelinemodel3x\n",
    "#Hint - what has changed from the last time we ran a pipeline\n",
    "newpipelinemodel = pipeline.fit(newtrainingdf)\n",
    "\n",
    "### predict on test1_3x data\n",
    "newpredictions = newpipelinemodel.transform(newtest1df)\n",
    "\n",
    "\n",
    "### look at result by calling show() on the new prediction data frame, prediction3x\n",
    "newpredictions.select('label','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Save model for y = -x1 - 5x2 - 10x3 to Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_path = 'https://ibm-watson-ml.mybluemix.net'\n",
    "instance_id = '***'\n",
    "username = '***'\n",
    "password = '***'\n",
    "yourname = 'Enter Your Name Here'\n",
    "\n",
    "modelname = modelname = name + ' y = -x1 - 5x2 - 10x3'\n",
    "\n",
    "ml_repository_client = MLRepositoryClient(service_path)\n",
    "ml_repository_client.authorize(username, password)\n",
    "\n",
    "### build model artifact by pasing model pipeline, name, and training data as parameters\n",
    "model_artifact = MLRepositoryArtifact(pipelinemodel, name=modelname, training_data=training3x)\n",
    "saved_model = ml_repository_client.models.save(model_artifact)\n",
    "\n",
    "print saved_model.uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Using a Watson Machine Learning Model\n",
    "\n",
    "Ask a partner for the information below, including the model uid for their y=3x model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service_path = 'https://ibm-watson-ml.mybluemix.net'\n",
    "partner_instance_id = '***'\n",
    "partner_username = '***'\n",
    "partner_password = '***'\n",
    "partner_saved_model_uid = '***'\n",
    "\n",
    "ml_repository_client = MLRepositoryClient(service_path)\n",
    "ml_repository_client.authorize(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load this model and print out it's name to verify it is from your partner's WML service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel = ml_repository_client.models.get(partner_saved_model_uid)\n",
    "print str(loadedModel.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel.model_instance().transform(test2_3x).select('x','label','prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
