{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}, "language_info": {"version": "2.7.11", "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Machine Learning Tutorials with Watson Machine Learning\n### Part 2 - Multivariate Linear Regression "}, {"metadata": {}, "cell_type": "markdown", "source": "for use in IBM Data Science Experience"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.1 Create data frames"}, {"execution_count": null, "cell_type": "code", "source": "import random\nimport numpy as np\nimport pylab as pl\n\nfrom pyspark.sql.types import *\nfrom pyspark.ml.linalg import SparseVector, VectorUDT\nfrom pyspark.ml import Pipeline, Model\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nfrom repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact\n\nimport urllib3, requests, json", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "x1 = [num / 1.0 for num in range(100)]\nx2 = [num / 1.0 for num in range(100)]\nx3 = [num / 1.0 for num in range(100)]\ny = range(100)\n\n#creating a noisy function to y = x1 + 2x2 + 3x3\nfor i in range(0,100):\n    y[i] = x1[i] + random.random() * random.uniform(-1.5,1.5) + \\\n           2 * x2[i] + random.random() * random.uniform(-1.5,1.5) + \\\n           3 * x3[i] + random.random() * random.uniform(-1.5,1.5)\n\nxytuple = zip(y,x1,x2,x3)\n\n#defining schema for spark dataframe\nschema = StructType([\n    StructField(\"label\", DoubleType(), True),\n    StructField(\"x1\", DoubleType(), True),\n    StructField(\"x2\", DoubleType(), True),\n    StructField(\"x3\", DoubleType(), True)\n])\n\n#creating spark dataframe, splitting to three groups\ndf = sqlContext.createDataFrame(xytuple, schema)\ntrainingdf, test1df, test2df = df.randomSplit([0.8, 0.1, 0.1])\n\ntrainingdf.show()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Create Spark ML Pipeline\ngroup x values into single feature vector this will be beneficial in the next exercise, involving multiple features"}, {"execution_count": null, "cell_type": "code", "source": "featuresvector = VectorAssembler(inputCols=[\"x1\", \"x2\", \"x3\"], outputCol=\"features\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "create linear regression instance"}, {"execution_count": null, "cell_type": "code", "source": "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "multiple ML steps can be executed at once by utilizing a pipeline, here we build out a pipeline that combines features in a single vector, then executes a linear regression model on the resulting (label, feature). We then run this pipeline using our training data"}, {"execution_count": null, "cell_type": "code", "source": "pipeline = Pipeline(stages =[featuresvector, lr])\n#pipelinemodel = pipeline.fit(trainingdf)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3 Use model on test1 dataframe"}, {"execution_count": null, "cell_type": "code", "source": "predictions = pipelinemodel.transform(test1df)\npredictions.select('label','prediction').show()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.4 Exercise\n#### The code above perform a linear regression training on data of the order y=x1 + 2x2 + 3x3 , plus some noise. Create data frames and deploy a model on the function y = -x1 - 5x2 - 10x3\n\n#### From the above code, what needs to be changed, what can stay the same?"}, {"execution_count": null, "cell_type": "code", "source": "### create data the looks like y = -x1 - 5x2 - 10x3, plus some noise\nx1 = [num / 1.0 for num in range(100)]\nx2 = [num / 1.0 for num in range(100)]\nx3 = [num / 1.0 for num in range(100)]\ny = range(100)\n\n#creating a noisy function to y = -x1 - 5x2 - 10x3\nfor i in range(0,100):\n    y[i] = x1[i] + random.random() * random.uniform(-1.5,1.5) - \\\n           5 * x2[i] + random.random() * random.uniform(-1.5,1.5) - \\\n           10 * x3[i] + random.random() * random.uniform(-1.5,1.5)\n            \nxytuple = zip(y,x1,x2,x3)\n\n### convert to three pyspark dataframes: newtrainingdf, newtest1df, & newtest2df\n#defining schema for spark dataframe\nschema = StructType([\n    StructField(\"label\", DoubleType(), True),\n    StructField(\"x1\", DoubleType(), True),\n    StructField(\"x2\", DoubleType(), True),\n    StructField(\"x3\", DoubleType(), True)\n])\n\n#creating spark dataframe, splitting to three groups\ndf = sqlContext.createDataFrame(xytuple, schema)\nnewtrainingdf, newtest1df, newtest2df = df.randomSplit([0.8, 0.1, 0.1])\n\nnewtrainingdf.show()\n", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "### build model pipeline, newpipelinemodel\n#Hint - what has changed from the last time we ran a pipeline\n\n\n### predict on test1_3x data\n\n\n\n### look at result by calling show() on the new prediction data frame, prediction3x\n", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.5 Save model for y = -x1 - 5x2 - 10x3 to Watson Machine Learning"}, {"execution_count": null, "cell_type": "code", "source": "service_path = 'https://ibm-watson-ml.mybluemix.net'\ninstance_id = '***'\nusername = '***'\npassword = '***'\nyourname = 'Enter Your Name Here'\n\nmodelname = modelname = name + ' y = -x1 - 5x2 - 10x3'\n\nml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)\n\n### build model artifact by pasing model pipeline, name, and training data as parameters\nmodel_artifact = MLRepositoryArtifact(pipelinemodel, name=modelname, training_data=training3x)\nsaved_model = ml_repository_client.models.save(model_artifact)\n\nprint saved_model.uid", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.6 Using a Watson Machine Learning Model\n\nAsk a partner for the information below, including the model uid for their y=3x model"}, {"execution_count": null, "cell_type": "code", "source": "service_path = 'https://ibm-watson-ml.mybluemix.net'\npartner_instance_id = '***'\npartner_username = '***'\npartner_password = '***'\npartner_saved_model_uid = '***'\n\nml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "load this model and print out it's name to verify it is from your partner's WML service"}, {"execution_count": null, "cell_type": "code", "source": "loadedModel = ml_repository_client.models.get(partner_saved_model_uid)\nprint str(loadedModel.name)", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "loadedModel.model_instance().transform(test2_3x).select('x','label','prediction').show()", "metadata": {}, "outputs": []}], "nbformat": 4, "nbformat_minor": 1}